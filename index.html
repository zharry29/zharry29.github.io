<!DOCTYPE html>
<html>

<head>
  <title>Li "Harry" Zhang</title>
  <meta charset="UTF-8">
  <meta name="viewport" content="width=device-width, initial-scale=1">
  <link rel="icon" href="image/favicon.png" sizes="16x16">
  <link rel="stylesheet" href="./w3.css">
  <link rel='stylesheet' href='https://fonts.googleapis.com/css?family=Roboto'>
  <link rel="stylesheet" href="https://cdnjs.cloudflare.com/ajax/libs/font-awesome/4.7.0/css/font-awesome.min.css">
  <style>
  html,
  body,
  h1,
  h2,
  h3,
  h4,
  h5,
  h6 {
    font-family: "Roboto", sans-serif
  }
  
  li {
    margin: 5px 0;
  }

  img {
    max-width: 100%;
    max-height: 100%;
  }

  iframe {
    aspect-ratio: 16 / 9;
    height: auto;
    width: 100%;
  }
  
  .tooltip {
    position: relative;
    display: inline-block;
    border-bottom: 1px dotted black;
  }
  
  .tooltip .tooltiptext {
    visibility: hidden;
    width: 140px;
    background-color: black;
    color: #fff;
    text-align: center;
    border-radius: 6px;
    padding: 5px 0;
    /* Position the tooltip */
    position: absolute;
    z-index: 1;
  }
  
  .tooltip:hover .tooltiptext {
    visibility: visible;
  }
  
  .hidden {
    display: none;
  }
  
  .button {
    background-color: Transparent;
    background-repeat: no-repeat;
    border: none;
    cursor: pointer;
    overflow: hidden;
    outline: none;
  }
  
  a {
    color: #9900cc;
    text-decoration: none;
  }
  
  .fab {
    position: relative;
    top: 3px;
  }
  
  .help {
    cursor: help;
  }
  </style>
  <!-- Place your kit's code here -->
  <script src="https://kit.fontawesome.com/e86b39c03c.js" crossorigin="anonymous"></script>
  <script>
  function toggleMoreResearch() {
    var x = document.getElementById("more_research");
    var y = document.getElementById("show_more_button_res");
    var z = document.getElementById("show_less_button_res");
    x.style.display = "block";
    y.style.display = "none";
    z.style.display = "block";
  }

  function toggleCollapseResearch() {
    var x = document.getElementById("more_research");
    var y = document.getElementById("show_more_button_res");
    var z = document.getElementById("show_less_button_res");
    x.style.display = "none";
    y.style.display = "block";
    z.style.display = "none";
  }
  
  function toggleMoreActivities() {
    var x = document.getElementById("more_activities");
    var y = document.getElementById("show_more_button_act");
    var z = document.getElementById("show_less_button_act");
    x.style.display = "block";
    y.style.display = "none";
    z.style.display = "block";
  }

  function toggleCollapseActivities() {
    var x = document.getElementById("more_activities");
    var y = document.getElementById("show_more_button_act");
    var z = document.getElementById("show_less_button_act");
    x.style.display = "none";
    y.style.display = "block";
    z.style.display = "none";
  }

  function toggle(bib) {
    var x = document.getElementById(bib);
    if(x.style.display === "block") {
      x.style.display = "none";
    } else {
      x.style.display = "block";
    }
  }
  </script>
</head>

<body class="w3-light-grey">
  <!-- Page Container -->
  <div class="w3-content w3-margin-top" style="max-width:1400px;">
    <!-- The Grid -->
    <div class="w3-row-padding">
      <!-- Left Column -->
      <div class="w3-third">
        <div class="w3-white w3-text-grey w3-card-4">
          <div class="w3-display-container">
            <!--
          <img src="avatar.jpg" style="width:100%" alt="Avatar">
    --><img src="image/drums.jpg" style="width:100%" alt="Avatar"> </div>
          <center>
            <h2><font face="georgia"> Li "Harry" Zhang &nbsp;</font><b>张力 </b></h2></center>
          <div class="w3-container">
            <hr>
            <p class="w3-large w3-text-theme"><b><i class="fa fa-user fa-fw w3-margin-right w3-text-purple"></i>About Me</b></p>
            <p><font color='black'>
              <font color='black'>I am an assistant professor at Drexel University focusing on Natural Language Processing and Artificial Intelligence. I'm interested in planning and reasoning using Large Language Models. I earned my PhD (<a href="https://arxiv.org/abs/2408.16098" target="_blank">thesis</a>) from the University of Pennsylvania, having the honor to be mentored by <a href="http://www.cis.upenn.edu/~ccb/" target="_blank">Prof. Chris Callison-Burch</a>, with a thesis committee chaired by <a href="https://www.cis.upenn.edu/~danroth/" target="_blank">Prof. Dan Roth</a>. I earned my BS from the University of Michigan in 2018, mentored by <a href="https://web.eecs.umich.edu/~mihalcea/" target="_blank">Prof. Rada Mihalcea</a> and <a href="http://www.cs.yale.edu/homes/radev/" target="_blank">Prof. Dragomir Radev</a>.</font>
              <br><br>
              I am hiring either paid and unpaid, either remote or in-person research assistants or interns with the goal of conversion to a PhD student. Those interested should <a href="https://forms.gle/uvUjbq3xcDpxQzva9" target="_blank">fill out this form</a>. I cannot respond to emails on this matter.
              <br><br>
              <a href="docs/CV.pdf" target="_blank"><span class="w3-tag w3-purple w3-round w3-hover-opacity">CV</span></a> <a href="https://www.semanticscholar.org/author/72436283" target="_blank"><button class='button'><span class="w3-tag w3-purple w3-round w3-hover-opacity">Semantic Scholar</span></button></a> <a href="https://scholar.google.com/citations?user=_VLzlBIAAAAJ" target="_blank"><button class='button'><span class="w3-tag w3-purple w3-round w3-hover-opacity">Google Scholar</span></button></a> 
            <p><i class="fa fa-envelope fa-fw w3-margin-right w3-large w3-text-purple"></i>Harry.Zhang@drexel.edu</p>
            
            <!--
            <p><i class="fa fa-university fa-fw w3-margin-right w3-large w3-text-purple"></i>University of Pennsylvania</p>
            <p><i class="fa fa-home fa-fw w3-margin-right w3-large w3-text-purple"></i>Shenzhen, China</p>-->
            <hr>
            <p class="w3-large w3-text-theme"><b><i class="fa fa-school fa-fw w3-margin-right w3-text-purple"></i>Affiliations</b></p>
            <div class="w3-container">
              <h5 class="w3-text-black">Drexel University<a href="https://drexel.edu/" target="_blank"><img src="image/drexel_logo.png" alt="drexel logo" style="float:right;width:80px;height:80px;"></a></h5>
              <h6 class="w3-text-purple"><i class="fa fa-graduation-cap fa-fw w3-margin-right"></i>Assistant Professor; <br>Dec 2024 to Present</h6>
              <br> </div>
            <div class="w3-container">
              <h5 class="w3-text-black">University of Pennsylvania<a href="https://www.upenn.edu/" target="_blank"><img src="image/upenn-logo.png" alt="upenn logo" style="float:right;width:80px;height:80px;"></a></h5>
              <h6 class="w3-text-purple"><i class="fa fa-graduation-cap fa-fw w3-margin-right"></i>Ph.D.; Aug 2019 to Aug 2024</h6>
              <br> </div>
            <div class="w3-container">
              <h5 class="w3-text-black">Allen Institute for Artifical Intelligence<a href="https://allenai.org" target="_blank"><img src="image/AI2_logo.png" alt="AI2" style="float:right;height:40px;"></a></h5>
              <h6 class="w3-text-purple"><i class="fa fa-suitcase fa-fw w3-margin-right"></i>Research Intern; <br>April 2023 to Dec 2023</h6>
              <!--<p>I did NLP research and software development on semantic role labeling, and previously, text simplification.</p>-->
              <br> </div>
            <div class="w3-container">
              <h5 class="w3-text-black">IBM Research<a href="https://research.ibm.com" target="_blank"><img src="image/ibm-logo.05dc870.png" alt="IBM Research" style="float:right;height:20px;"></a></h5>
              <h6 class="w3-text-purple"><i class="fa fa-suitcase fa-fw w3-margin-right"></i>Research Intern; May 2021 to Aug 2021, <br>April 2019 to June 2019</h6>
              <!--<p>I did NLP research and software development on semantic role labeling, and previously, text simplification.</p>-->
              <br> </div>
              <div class="w3-container">
                <h5 class="w3-text-black">University of Michigan<img src="image/umich-logo.png" alt="umich logo" style="float:right;width:80px;height:80px;"></h5>
                <h6 class="w3-text-purple"><i class="fa fa-graduation-cap fa-fw w3-margin-right"></i>B.S.E.; Aug 2015 to Dec 2018</h6>
                <br> </div>
                <div class="w3-container">
                  <h5 class="w3-text-black">Goldman Sachs<a href="https://www.goldmansachs.com" target="_blank"><img src="image/GoldmanSachsLogo.jpg" alt="Goldman Sachs" style="float:right;height:40px;"></a></h5>
                  <h6 class="w3-text-purple"><i class="fa fa-suitcase fa-fw w3-margin-right"></i>Summer Analyst; <br>May 2017 to Aug 2017</h6>
                  <!--<p>I performed software engineering, data analytics and machine learning.</p>-->
                  <br> </div>
              <div class="w3-container">
                <h5 class="w3-text-black">Shenzhen Middle School<img src="image/sms-logo.jpg" alt="SMS logo" style="float:right;width:80px;height:80px;"></h5>
                <h6 class="w3-text-purple"><i class="fa fa-graduation-cap fa-fw w3-margin-right"></i>High School Diploma; <br>Sept 2012 to Jun 2015</h6>
                </div>
              <hr>
            <p class="w3-large w3-text-theme"><b><i class="fa fa-book fa-fw w3-margin-right w3-text-purple"></i>Mentorship and Teaching</b></p>
            <div class="w3-container">
              <h5 class="w3-text-black">HazLab at Drexel</h5>
              <h6 class="w3-text-black"><i class="fa fa-user fa-fw w3-margin-right"></i><a href="https://www.linkedin.com/in/cassie-huang-35b396185" target="_blank">Cassie Huang</a> (PhD student)</h6>
              <h6 class="w3-text-black"><i class="fa fa-user fa-fw w3-margin-right"></i><a href="https://www.linkedin.com/in/liancheng-gong-ab2137173" target="_blank">Krystal Gong</a> (intern)</h6>
              <h6 class="w3-text-black"><i class="fa fa-user fa-fw w3-margin-right"></i><a href="https://in.linkedin.com/in/prakashkagitha" target="_blank">Prabhu Prakash Kagitha</a> (MS student)</h6>
              <h6 class="w3-text-black"><i class="fa fa-user fa-fw w3-margin-right"></i><a href="https://www.linkedin.com/in/rikhil-amonkar-06223532b" target="_blank">Rikhil Amonkar</a> (UG student)</h6>
              <h5 class="w3-text-black">Past Mentees at Penn</h5>
              <h6 class="w3-text-black"><i class="fa fa-user fa-fw w3-margin-right"></i><a href="https://tianyi0608.github.io/tianyizhang/" target="_blank">Tianyi Zhang</a></h6>
              <h6 class="w3-text-black"><i class="fa fa-user fa-fw w3-margin-right"></i><a href="seacowx.github.io " target="_blank">Hainiu Xu</a>, King's College London</h6>
              <h6 class="w3-text-black"><i class="fa fa-user fa-fw w3-margin-right"></i><a href="https://joeyhou.github.io/" target="_blank">Zhaoyi Hou</a>, University of Pittsburg</h6>
              <h6 class="w3-text-black"><i class="fa fa-user fa-fw w3-margin-right"></i><a href="https://www.linkedin.com/in/jeffrey-young-min-cho-888105180" target="_blank">Young-Min Cho</a>, University of Pennsylvania</h6>
              </div>
            <div class="w3-container">
              <h5 class="w3-text-black">Teaching</h5>
              <p><i class="fa fa-book fa-fw w3-margin-right"></i>Instructor: CS T780-001: Applied NLP (Spring 2025 at Drexel)</p>
              <p><i class="fa fa-book fa-fw w3-margin-right"></i>TA: CIS 530: Computational Linguistics (Winter, Fall 2020 at Penn)</p>
              <p><i class="fa fa-book fa-fw w3-margin-right"></i>TA: EECS 595: Natural Language Processing (Fall 2018 at Michigan)</p>
              <p><i class="fa fa-book fa-fw w3-margin-right"></i>TA: EECS 280: Programming and Introductory Data Structures (Winter, Fall 2016 at Michigan)</p>
              </div>
            <hr>
            <p class="w3-large w3-text-theme"><b><i class="fas fa-scroll fa-fw w3-margin-right w3-text-purple"></i>Service</b></p>
            <p>I have reviewed more than 50 papers of and chaired for many NLP conferences and workshops.</p>
            <div class="w3-container">
            <h6 class="w3-text-black"><i class="fa fa-chair fa-fw w3-margin-right"></i>Area Chair of <a href='https://openreview.net/group?id=aclweb.org/ACL/ARR/2025/February'  target="_blank">ARR Feb 2025</a> / <a href='https://2025.aclweb.org/' target="_blank">ACL 2025</a>, <a href='https://openreview.net/group?id=aclweb.org/ACL/ARR/2024/December'  target="_blank">ARR Dec 2024</a>, <a href='https://coling2025.org/'  target="_blank">COLING 2025</a>, <a href='https://openreview.net/group?id=aclweb.org/ACL/ARR/2024/August'  target="_blank">ARR Aug 2024</a>, <a href='https://openreview.net/group?id=aclweb.org/ACL/ARR/2024/June'  target="_blank">ARR Jun 2024</a> / <a href='https://2024.emnlp.org/' target="_blank">EMNLP 2024</a>, <a href='https://openreview.net/group?id=aclweb.org/ACL/ARR/2024/February'  target="_blank">ARR Feb 2024</a> / <a href='https://2024.aclweb.org/' target="_blank">ACL 2024</a></h6>
            <h6 class="w3-text-black"><i class="fa fa-chair fa-fw w3-margin-right"></i>Session Chair of <a href='https://2024.aclweb.org/' target="_blank">ACL 2024</a>, <a href='http://www.aacl2020.org/'  target="_blank">AACL-IJCNLP 2020</a></h6>
            <h6 class="w3-text-black"><i class="fa fa-chair fa-fw w3-margin-right"></i>Program Chair of <a href='https://www.mascsll.org/'  target="_blank">MASC-SLL 2023</a>, <a href='https://www.mascsll.org/'  target="_blank">MASC-SLL 2021</a></h6>
            <h6 class="w3-text-black"><i class="fa fa-pen-nib fa-fw w3-margin-right"></i>Reviewer of <a href='https://lrec-coling-2024.org/2nd-call-for-papers/'  target="_blank">LREC-COLING 2024</a>, <a href='https://2023.emnlp.org/'  target="_blank">EMNLP 2023</a>, <a href='https://2023.aclweb.org/' target="_blank">ACL 2023</a>, <a href='https://openreview.net/group?id=aclweb.org/ACL/ARR'  target="_blank">ARR Mar 2022</a>, <a href='https://www.dashworkshops.org/'  target="_blank">DaSH Workshop @ EMNLP 2022</a>, <a href='https://coling2022.org/coling'  target="_blank">COLING 2022</a>, <a href='https://lrec2022.lrec-conf.org/en/'  target="_blank">LREC 2022</a>, <a href='https://openreview.net/group?id=aclweb.org/ACL/ARR'  target="_blank">ARR Nov 2021</a>, <a href='https://coling2020.org/'  target="_blank">COLING 2020</a>, <a href='https://www.journals.elsevier.com/computer-speech-and-language'  target="_blank">Computer Speech and Language 2018</a> </h6>
          </div>
            <br> </div>
        </div>
        <br>
        <!-- End Left Column -->
      </div>
      <!-- Right Column -->
      <div class="w3-twothird">
        <div class="w3-container w3-card-2 w3-white w3-margin-bottom">
          <h2 class="w3-text-grey w3-padding-16"><i class="fa fa-robot fa-fw w3-margin-right w3-xxlarge w3-text-purple"></i>Research</h2>
          <div class="w3-container">
            <a name="pub6"></a>
            <h5 class="w3-text-gray"><b>Primary Work: Executable and Trustworthy Planning with LLMs</b></h5>
          <p>
            While large language models (LLM) can provide decent instructions, they are far from able to come up an executable and trustworthy plan for a particular user or agent, grounding to their specific situation and needs. To address this, I advocate for the neurosymbolic methodology of using LLM as a code generator to create a formal representation of the planning environment. In conjunction with tools in classical AI planning, a plan can be found deterministically and faithfully.
          </p>
        <hr>
        <p>
          My primary efforts lie in using LLMs to generate formal language, such as <a href="https://planning.wiki/guide/whatis/pddl" target="_blank">PDDL</a> that describes the planning environment. 
        </p>
        <p class="w3-text-gray">[30] <i>On the Limit of Language Models as Planning Formalizers</i>; Cassie Huang<span class="tooltip">^<span class="tooltiptext">Mentored student</span></span> and <b>Li Zhang</b>; in arxiv.<a href="https://arxiv.org/abs/2412.09879" target="_blank"><span class="w3-tag w3-white w3-border w3-border-black w3-text-black w3-round w3-hover-opacity">Paper</span></a> <span class="help w3-tag w3-white w3-border w3-border-black w3-text-black w3-round w3-hover-opacity" onclick="toggle('pddleval')">BibTeX</span> <a href="https://github.com/CassieHuang22/llm-as-pddl-formalizer" target="_blank"><span class="w3-tag w3-white w3-border w3-border-black w3-text-black w3-round w3-hover-opacity">Repo</span></a></p>
        <div id="pddleval" class='hidden'> <pre style="overflow:auto">
@misc{huang2024limitlanguagemodelsplanning,
  title={On the Limit of Language Models as Planning Formalizers}, 
  author={Cassie Huang and Li Zhang},
  year={2024},
  eprint={2412.09879},
  archivePrefix={arXiv},
  primaryClass={cs.CL},
  url={https://arxiv.org/abs/2412.09879}, 
          </pre> </div>
        <p class="w3-text-gray">[29] <i>PDDLEGO: Iterative Planning in Textual Environments</i>; <b>Li Zhang</b>, Peter Jansen, Peter Clark, Chris Callison-Burch and Niket Tandon; in <a href="https://sites.google.com/view/starsem2024" target="_blank">*SEM 2024</a>.<a href="https://aclanthology.org/2024.starsem-1.17/" target="_blank"><span class="w3-tag w3-white w3-border w3-border-black w3-text-black w3-round w3-hover-opacity">Paper</span></a> <span class="help w3-tag w3-white w3-border w3-border-black w3-text-black w3-round w3-hover-opacity" onclick="toggle('pddlego')">BibTeX</span> <a href="https://github.com/zharry29/nl-to-pddl" target="_blank"><span class="w3-tag w3-white w3-border w3-border-black w3-text-black w3-round w3-hover-opacity">Repo</span></a></p>
        <div id="pddlego" class='hidden'> <pre style="overflow:auto">
@inproceedings{zhang-etal-2024-pddlego,
  title = "{PDDLEGO}: Iterative Planning in Textual Environments",
  author = "Zhang, Li  and
    Jansen, Peter  and
    Zhang, Tianyi  and
    Clark, Peter  and
    Callison-Burch, Chris  and
    Tandon, Niket",
  editor = "Bollegala, Danushka  and
    Shwartz, Vered",
  booktitle = "Proceedings of the 13th Joint Conference on Lexical and Computational Semantics (*SEM 2024)",
  month = jun,
  year = "2024",
  address = "Mexico City, Mexico",
  publisher = "Association for Computational Linguistics",
  url = "https://aclanthology.org/2024.starsem-1.17",
  pages = "212--221",
  abstract = "Planning in textual environments have been shown to be a long-standing challenge even for current models. A recent, promising line of work uses LLMs to generate a formal representation of the environment that can be solved by a symbolic planner. However, existing methods rely on a fully-observed environment where all entity states are initially known, so a one-off representation can be constructed, leading to a complete plan. In contrast, we tackle partially-observed environments where there is initially no sufficient information to plan for the end-goal. We propose PDDLEGO that iteratively construct a planning representation that can lead to a partial plan for a given sub-goal. By accomplishing the sub-goal, more information is acquired to augment the representation, eventually achieving the end-goal. We show that plans produced by few-shot PDDLEGO are 43{\%} more efficient than generating plans end-to-end on the Coin Collector simulation, with strong performance (98{\%}) on the more complex Cooking World simulation where end-to-end LLMs fail to generate coherent plans (4{\%}).",
}
          </pre> </div>
        <p class="w3-text-gray">[28] <i>PROC2PDDL: Open-Domain Planning Representations from Texts</i>; Tianyi Zhang<span class="tooltip">*<span class="tooltiptext">Equal contribution</span></span><span class="tooltip">^<span class="tooltiptext">Mentored student</span></span>, <b>Li Zhang</b><span class="tooltip">*<span class="tooltiptext">Equal contribution</span></span>, Zhaoyi Hou<span class="tooltip">^<span class="tooltiptext">Mentored student</span></span>, Ziyu Wang<span class="tooltip">^<span class="tooltiptext">Mentored student</span></span>, Yuling Gu, Peter Clark, Chris Callison-Burch and Niket Tandon; in <a href="https://nl-reasoning-workshop.github.io/" target="_blank">ACL 2024 2st Workshop on Natural Language Reasoning and Structured Explanations</a>.<a href="https://aclanthology.org/2024.nlrse-1.2/" target="_blank"><span class="w3-tag w3-white w3-border w3-border-black w3-text-black w3-round w3-hover-opacity">Paper</span></a> <span class="help w3-tag w3-white w3-border w3-border-black w3-text-black w3-round w3-hover-opacity" onclick="toggle('proc2pddl')">BibTeX</span> <a href="https://github.com/zharry29/proc2pddl" target="_blank"><span class="w3-tag w3-white w3-border w3-border-black w3-text-black w3-round w3-hover-opacity">Repo</span></a></p>
        <div id="proc2pddl" class='hidden'> <pre style="overflow:auto">
@inproceedings{zhang-etal-2024-proc2pddl,
  title = "PROC2PDDL: Open-Domain Planning Representations from Texts",
  author = "Zhang, Tianyi and Zhang, Li  and Hou, Zhaoyi and Wang, Ziyu and Gu, Yuling and Clark, Peter and Callison-Burch, Chris and Tandon, Niket",
  booktitle = "Proceedings of the 2st Workshop on Natural Language Reasoning and Structured Explanations (NLRSE)",
  month = aug,
  year = "2024",
  address = "Bangkok, Thailand",
  publisher = "Association for Computational Linguistics",
}
</pre> </div> 
<p class="w3-text-gray">[20] <i>Faithful Chain of Thought Reasoning</i>; Qing Lyu<span class="tooltip">*<span class="tooltiptext">Equal contribution</span></span>, Shreya Havaldar<span class="tooltip">*<span class="tooltiptext">Equal contribution</span></span>, Adam Stein<span class="tooltip">*<span class="tooltiptext">Equal contribution</span></span>, <b>Li Zhang</b>, Delip Rao, Eric Wong, Marianna Apidianaki and Chris Callison-Burch; in <a href="http://www.ijcnlp-aacl2023.org/" target="_blank">IJCNLP-AACL 2023</a>. Won <a href="https://www.afnlp.org/conferences/ijcnlp2023/wp/best-paper-awards/" target="_blank">Area Chair Award</a>.<a href="https://aclanthology.org/2023.ijcnlp-main.20/" target="_blank"><span class="w3-tag w3-white w3-border w3-border-black w3-text-black w3-round w3-hover-opacity">Paper</span></a> <span class="help w3-tag w3-white w3-border w3-border-black w3-text-black w3-round w3-hover-opacity" onclick="toggle('fcotbib')">BibTeX</span> <a href="https://github.com/veronica320/Faithful-COT" target="_blank"><span class="w3-tag w3-white w3-border w3-border-black w3-text-black w3-round w3-hover-opacity">Repo</span></a></p>
<div id="fcotbib" class='hidden'> <pre style="overflow:auto">
@inproceedings{lyu-etal-2023-faithful,
  title = "Faithful Chain-of-Thought Reasoning",
  author = "Lyu, Qing  and
  Havaldar, Shreya  and
  Stein, Adam  and
  Zhang, Li  and
  Rao, Delip  and
  Wong, Eric  and
  Apidianaki, Marianna  and
  Callison-Burch, Chris",
  editor = "Park, Jong C.  and
  Arase, Yuki  and
  Hu, Baotian  and
  Lu, Wei  and
  Wijaya, Derry  and
  Purwarianti, Ayu  and
  Krisnadhi, Adila Alfa",
  booktitle = "Proceedings of the 13th International Joint Conference on Natural Language Processing and the 3rd Conference of the Asia-Pacific Chapter of the Association for Computational Linguistics (Volume 1: Long Papers)",
  month = nov,
  year = "2023",
  address = "Nusa Dua, Bali",
  publisher = "Association for Computational Linguistics",
  url = "https://aclanthology.org/2023.ijcnlp-main.20",
  pages = "305--329",
}
    </pre> </div> 
            

    </div>            <div id='show_more_button_res'>
        <button onclick="toggleMoreResearch()" class='button'><span class="w3-tag w3-purple w3-round w3-hover-opacity">See all publications</span></button>
      </div>
      <div id="more_research" class='hidden'>
        <hr>
      <div class="w3-container">
        <p class="w3-text-gray">[31] DynaCode: A Dynamic Complexity-Aware Code Benchmark for Evaluating Large Language Models in Code Generation; Wenhao Hu, Jinhao Duan, Chunchen Wei, <b>Li Zhang</b>, Yue Zhang, Kaidi Xu; in arxiv.<a href="https://arxiv.org/abs/2503.10452/" target="_blank"><span class="w3-tag w3-white w3-border w3-border-black w3-text-black w3-round w3-hover-opacity">Paper</span></a> <span class="help w3-tag w3-white w3-border w3-border-black w3-text-black w3-round w3-hover-opacity" onclick="toggle('dynacode')">BibTeX</span></p>
        <div id="dynacode" class='hidden'> <pre style="overflow:auto">
@misc{hu2025dynacodedynamiccomplexityawarecode,
  title={DynaCode: A Dynamic Complexity-Aware Code Benchmark for Evaluating Large Language Models in Code Generation}, 
  author={Wenhao Hu and Jinhao Duan and Chunchen Wei and Li Zhang and Yue Zhang and Kaidi Xu},
  year={2025},
  eprint={2503.10452},
  archivePrefix={arXiv},
  primaryClass={cs.CL},
  url={https://arxiv.org/abs/2503.10452}, 
}
          </pre> </div>
    <p class="w3-text-gray">[27] <i>Calibrating Large Language Models with Sample Consistency</i>; Qing Lyu<span class="tooltip">*<span class="tooltiptext">Equal contribution</span></span>, Kumar Shridhar<span class="tooltip">*<span class="tooltiptext">Equal contribution</span></span>, Chaitanya Malaviya, Li Zhang, Yanai Elazar, Niket Tandon, Marianna Apidianaki, Mrinmaya Sachan and Chris Callison-Burch; in <a href="https://aaai.org/conference/aaai/aaai-25/" target="_blank">AAAI 2025</a>.<a href="https://arxiv.org/abs/2402.13904" target="_blank"><span class="w3-tag w3-white w3-border w3-border-black w3-text-black w3-round w3-hover-opacity">Paper</span></a> <span class="help w3-tag w3-white w3-border w3-border-black w3-text-black w3-round w3-hover-opacity" onclick="toggle('consistencybib')">BibTeX</span></p>
    <div id="consistencybib" class='hidden'> <pre style="overflow:auto">
  @misc{lyu2024calibrating,
    title={Calibrating Large Language Models with Sample Consistency}, 
    author={Qing Lyu and Kumar Shridhar and Chaitanya Malaviya and Li Zhang and Yanai Elazar and Niket Tandon and Marianna Apidianaki and Mrinmaya Sachan and Chris Callison-Burch},
    year={2024},
    eprint={2402.13904},
    archivePrefix={arXiv},
    primaryClass={cs.CL}
  }
  </pre> </div> 
  <p class="w3-text-gray">[26] <i>Tailoring with Targeted Precision: Edit-Based Agents for Open-Domain Procedure Customization</i>; Yash Kumar Lal, <b>Li Zhang</b>, Faeze Brahman, Bodhisattwa Prasad Majumder, Peter Clark, Niket Tandon; in Findings of <a href="https://2024.aclweb.org/" target="_blank">ACL 2024</a>.<a href="https://2024.aclweb.org/" target="_blank"><span class="w3-tag w3-white w3-border w3-border-black w3-text-black w3-round w3-hover-opacity">Paper</span></a> <span class="help w3-tag w3-white w3-border w3-border-black w3-text-black w3-round w3-hover-opacity" onclick="toggle('customize')">BibTeX</span></p>
        <div id="customize" class='hidden'> <pre style="overflow:auto">
@inproceedings{lal-etal-2024-tailoring,
  title = "Tailoring with Targeted Precision: Edit-Based Agents for Open-Domain Procedure Customization",
  author = "Lal, Yash Kumar  and
    Zhang, Li  and
    Brahman, Faeze  and
    Majumder, Bodhisattwa Prasad  and
    Clark, Peter  and
    Tandon, Niket",
  editor = "Ku, Lun-Wei  and
    Martins, Andre  and
    Srikumar, Vivek",
  booktitle = "Findings of the Association for Computational Linguistics: ACL 2024",
  month = aug,
  year = "2024",
  address = "Bangkok, Thailand",
  publisher = "Association for Computational Linguistics",
  url = "https://aclanthology.org/2024.findings-acl.921/",
  doi = "10.18653/v1/2024.findings-acl.921",
  pages = "15597--15611",
  abstract = "How-to procedures, such as how to plant a garden, are now used by millions of users, but sometimes need customizing to meet a user`s specific needs, e.g., planting a garden without pesticides. Our goal is to measure and improve an LLM`s ability to perform such customization. Our approach is to test several simple multi-LLM-agent architectures for customization, as well as an end-to-end LLM, using a new evaluation set, called CustomPlans, of over 200 WikiHow procedures each with a customization need. We find that a simple architecture with two LLM agents used sequentially performs best, one that edits a generic how-to procedure and one that verifies its executability, significantly outperforming (10.5{\%} absolute) an end-to-end prompted LLM. This suggests that LLMs can be configured reasonably effectively for procedure customization. This also suggests that multi-agent editing architectures may be worth exploring further for other customization applications (e.g. coding, creative writing) in the future."
}     
          </pre> </div>
          <p class="w3-text-gray">[25] <i>CLIN: A Continually Learning Language Agent for Rapid Task Adaptation and Generalization</i>; Bodhisattwa Prasad Majumder, Bhavana Dalvi Mishra, Peter Jansen, Oyvind Tafjord, Niket Tandon, <b>Li Zhang</b>, Chris Callison-Burch, Peter Clark; in <a href="https://colmweb.org/" target="_blank">COLM 2024</a>.<a href="https://arxiv.org/abs/2310.10134" target="_blank"><span class="w3-tag w3-white w3-border w3-border-black w3-text-black w3-round w3-hover-opacity">Paper</span></a> <span class="help w3-tag w3-white w3-border w3-border-black w3-text-black w3-round w3-hover-opacity" onclick="toggle('clin')">BibTeX</span> <a href="https://allenai.github.io/clin/" target="_blank"><span class="w3-tag w3-white w3-border w3-border-black w3-text-black w3-round w3-hover-opacity">Repo</span></a></p>
        <div id="clin" class='hidden'> <pre style="overflow:auto">
@misc{majumder2023clin,
  title={CLIN: A Continually Learning Language Agent for Rapid Task Adaptation and Generalization}, 
  author={Bodhisattwa Prasad Majumder and Bhavana Dalvi Mishra and Peter Jansen and Oyvind Tafjord and Niket Tandon and Li Zhang and Chris Callison-Burch and Peter Clark},
  year={2023},
  eprint={2310.10134},
  archivePrefix={arXiv},
  primaryClass={cs.CL}
}
          </pre> </div>
          <p class="w3-text-gray">[24] <i>Choice-75: A Dataset on Decision Branching in Script Learning</i>; Zhaoyi Joey Hou<span class="tooltip">^<span class="tooltiptext">Mentored student</span></span>, <b>Li Zhang</b>, Chris Callison-Burch; in <a href="lrec-coling-2024.org" target="_blank">LREC-COLING 2024</a>.<a href="https://aclanthology.org/2024.lrec-main.285/" target="_blank"><span class="w3-tag w3-white w3-border w3-border-black w3-text-black w3-round w3-hover-opacity">Paper</span></a> <span class="help w3-tag w3-white w3-border w3-border-black w3-text-black w3-round w3-hover-opacity" onclick="toggle('choice75bib')">BibTeX</span> <a href="https://github.com/JoeyHou/branching" target="_blank"><span class="w3-tag w3-white w3-border w3-border-black w3-text-black w3-round w3-hover-opacity">Repo</span></a></p>
          <div id="choice75bib" class='hidden'> <pre style="overflow:auto">
@inproceedings{hou-etal-2024-choice-75,
  title = "Choice-75: A Dataset on Decision Branching in Script Learning",
  author = "Hou, Zhaoyi  and
    Zhang, Li  and
    Callison-Burch, Chris",
  editor = "Calzolari, Nicoletta  and
    Kan, Min-Yen  and
    Hoste, Veronique  and
    Lenci, Alessandro  and
    Sakti, Sakriani  and
    Xue, Nianwen",
  booktitle = "Proceedings of the 2024 Joint International Conference on Computational Linguistics, Language Resources and Evaluation (LREC-COLING 2024)",
  month = may,
  year = "2024",
  address = "Torino, Italia",
  publisher = "ELRA and ICCL",
  url = "https://aclanthology.org/2024.lrec-main.285",
  pages = "3215--3223",
  abstract = "Script learning studies how daily events unfold. It enables machines to reason about narratives with implicit information. Previous works mainly consider a script as a linear sequence of events while ignoring the potential branches that arise due to people{'}s circumstantial choices. We hence propose Choice-75, the first benchmark that challenges intelligent systems to make decisions given descriptive scenarios, containing 75 scripts and more than 600 scenarios. We also present preliminary results with current large language models (LLM). Although they demonstrate overall decent performances, there is still notable headroom in hard scenarios.",
}
        </pre> </div> 
        <p class="w3-text-gray">[23] <i>OpenPI2.0: An Improved Dataset for Entity Tracking in Texts</i>; <b>Li Zhang</b>, Hainiu Xu<span class="tooltip">^<span class="tooltiptext">Mentored student</span></span>, Abhinav Kommula, Chris Callison-Burch and Niket Tandon; in <a href="https://2024.eacl.org/" target="_blank">EACL 2024</a>.<a href="https://aclanthology.org/2024.eacl-long.10/" target="_blank"><span class="w3-tag w3-white w3-border w3-border-black w3-text-black w3-round w3-hover-opacity">Paper</span></a> <span class="help w3-tag w3-white w3-border w3-border-black w3-text-black w3-round w3-hover-opacity" onclick="toggle('openpi20')">BibTeX</span> <a href="https://github.com/allenai/openpi-dataset/" target="_blank"><span class="w3-tag w3-white w3-border w3-border-black w3-text-black w3-round w3-hover-opacity">Repo</span></a></p>
        <div id="openpi20" class='hidden'> <pre style="overflow:auto">
@inproceedings{zhang-etal-2024-openpi2,
  title = "{O}pen{PI}2.0: An Improved Dataset for Entity Tracking in Texts",
  author = "Zhang, Li  and
    Xu, Hainiu  and
    Kommula, Abhinav  and
    Callison-Burch, Chris  and
    Tandon, Niket",
  editor = "Graham, Yvette  and
    Purver, Matthew",
  booktitle = "Proceedings of the 18th Conference of the European Chapter of the Association for Computational Linguistics (Volume 1: Long Papers)",
  month = mar,
  year = "2024",
  address = "St. Julian{'}s, Malta",
  publisher = "Association for Computational Linguistics",
  url = "https://aclanthology.org/2024.eacl-long.10",
  pages = "166--178",
  abstract = "Much texts describe a changing world (e.g., procedures, stories, newswires), and understanding them requires tracking how entities change. An earlier dataset, OpenPI, provided crowdsourced annotations of entity state changes in text. However, a major limitation was that those annotations were free-form and did not identify salient changes, hampering model evaluation. To overcome these limitations, we present an improved dataset, OpenPI2.0, where entities and attributes are fully canonicalized and additional entity salience annotations are added. On our fairer evaluation setting, we find that current state-of-the-art language models are far from competent. We also show that using state changes of salient entities as a chain-of-thought prompt, downstream performance is improved on tasks such as question answering and classical planning, outperforming the setting involving all related entities indiscriminately. We offer OpenPI2.0 for the continued development of models that can understand the dynamics of entities in text.",
}
          </pre> </div>
    <p class="w3-text-gray">[22] <i>Exploring the Curious Case of Code Prompts</i>; <b>Li Zhang</b><span class="tooltip">*<span class="tooltiptext">Equal contribution</span></span>, Liam Dugan<span class="tooltip">*<span class="tooltiptext">Equal contribution</span></span>, Hainiu Xu<span class="tooltip">^<span class="tooltiptext">Mentored student</span></span><span class="tooltip">*<span class="tooltiptext">Equal contribution</span></span> and Chris Callison-Burch; in <a href="https://nl-reasoning-workshop.github.io/" target="_blank">ACL 2023 1st Workshop on Natural Language Reasoning and Structured Explanations</a>.<a href="https://aclanthology.org/2023.nlrse-1.2/" target="_blank"><span class="w3-tag w3-white w3-border w3-border-black w3-text-black w3-round w3-hover-opacity">Paper</span></a> <span class="help w3-tag w3-white w3-border w3-border-black w3-text-black w3-round w3-hover-opacity" onclick="toggle('codexbib')">BibTeX</span> <a href="https://github.com/zharry29/curious_code_prompts" target="_blank"><span class="w3-tag w3-white w3-border w3-border-black w3-text-black w3-round w3-hover-opacity">Repo</span></a></p>
    <div id="codexbib" class='hidden'> <pre style="overflow:auto">
@inproceedings{zhang-etal-2023-exploring,
title = "Exploring the Curious Case of Code Prompts",
author = "Zhang, Li  and
Dugan, Liam  and
Xu, Hainiu  and
Callison-burch, Chris",
booktitle = "Proceedings of the 1st Workshop on Natural Language Reasoning and Structured Explanations (NLRSE)",
month = jun,
year = "2023",
address = "Toronto, Canada",
publisher = "Association for Computational Linguistics",
url = "https://aclanthology.org/2023.nlrse-1.2",
pages = "9--17",
abstract = "Recent work has shown that prompting language models with code-like representations of natural language leads to performance improvements on structured reasoning tasks. However, such tasks comprise only a small subset of all natural language tasks. In our work, we seek to answer whether or not code-prompting is the preferred way of interacting with language models in general. We compare code and text prompts across three popular GPT models (davinci, code-davinci-002, and text-davinci-002) on a broader selection of tasks (e.g., QA, sentiment, summarization) and find that with few exceptions, code prompts do not consistently outperform text prompts. Furthermore, we show that the style of code prompt has a large effect on performance for some (but not all) tasks and that fine-tuning on text instructions leads to better relative performance of code prompts.",
}

      </pre> </div> 
      <p class="w3-text-gray">[21] <i>Human-in-the-Loop Schema Induction</i>; Tianyi Zhang<span class="tooltip">^<span class="tooltiptext">Mentored student</span></span>, Isaac Tham, Zhaoyi Hou<span class="tooltip">^<span class="tooltiptext">Mentored student</span></span>, Jiaxuan Ren, Liyang Zhou<span class="tooltip">^<span class="tooltiptext">Mentored student</span></span>, Hainiu Xu<span class="tooltip">^<span class="tooltiptext">Mentored student</span></span>, <b>Li Zhang</b>, Lara J. Martin, Rotem Dror, Sha Li, Heng Ji, Martha Palmer, Susan Brown, Reece Suchocki, Chris Callison-Burch; in <a href="https://2023.aclweb.org/">ACL 2023</a> Demos.<a href="https://aclanthology.org/2023.acl-demo.1/" target="_blank"><span class="w3-tag w3-white w3-border w3-border-black w3-text-black w3-round w3-hover-opacity">Paper</span></a> <span class="help w3-tag w3-white w3-border w3-border-black w3-text-black w3-round w3-hover-opacity" onclick="toggle('schemabib')">BibTeX</span> <a href="https://www.kairos.jiaxuan.me/" target="_blank"><span class="w3-tag w3-white w3-border w3-border-black w3-text-black w3-round w3-hover-opacity">Demo</span></a> </p>
      <div id="schemabib" class='hidden'> 
        <pre style="overflow:auto">
@inproceedings{zhang-etal-2023-human,
  title = "Human-in-the-loop Schema Induction",
  author = "Zhang, Tianyi  and
    Tham, Isaac  and
    Hou, Zhaoyi  and
    Ren, Jiaxuan  and
    Zhou, Leon  and
    Xu, Hainiu  and
    Zhang, Li  and
    Martin, Lara  and
    Dror, Rotem  and
    Li, Sha  and
    Ji, Heng  and
    Palmer, Martha  and
    Brown, Susan Windisch  and
    Suchocki, Reece  and
    Callison-Burch, Chris",
  booktitle = "Proceedings of the 61st Annual Meeting of the Association for Computational Linguistics (Volume 3: System Demonstrations)",
  month = jul,
  year = "2023",
  address = "Toronto, Canada",
  publisher = "Association for Computational Linguistics",
  url = "https://aclanthology.org/2023.acl-demo.1",
  pages = "1--10",
  abstract = "Schema induction builds a graph representation explaining how events unfold in a scenario. Existing approaches have been based on information retrieval (IR) and information extraction (IE), often with limited human curation. We demonstrate a human-in-the-loop schema induction system powered by GPT-3. We first describe the different modules of our system, including prompting to generate schematic elements, manual edit of those elements, and conversion of those into a schema graph. By qualitatively comparing our system to previous ones, we show that our system not only transfers to new domains more easily than previous approaches, but also reduces efforts of human curation thanks to our interactive interface.",
}
    </pre> 
  </div> 
  
    <p class="w3-text-gray">[19] <i>Causal Reasoning of Entities and Events in Procedural Texts</i>; <b>Li Zhang</b><span class="tooltip">*<span class="tooltiptext">Equal contribution</span></span>, Hainiu Xu<span class="tooltip">*<span class="tooltiptext">Equal contribution</span></span><span class="tooltip">^<span class="tooltiptext">Mentored student</span></span>, Yue Yang, Shuyan Zhou, Weiqiu You, Manni Arora and Chris Callison-Burch; in Findings of <a href="https://2023.eacl.org/" target="_blank">EACL 2023</a>.<a href="https://aclanthology.org/2023.findings-eacl.31/" target="_blank"><span class="w3-tag w3-white w3-border w3-border-black w3-text-black w3-round w3-hover-opacity">Paper</span></a> <span class="help w3-tag w3-white w3-border w3-border-black w3-text-black w3-round w3-hover-opacity" onclick="toggle('eacl2023')">BibTeX</span> <a href="https://github.com/zharry29/causal_reasoning_of_entities_and_events" target="_blank"><span class="w3-tag w3-white w3-border w3-border-black w3-text-black w3-round w3-hover-opacity">Repo</span></a></p>
        <div id="eacl2023" class='hidden'> <pre style="overflow:auto">
@inproceedings{zhang-etal-2023-causal,
  title = "Causal Reasoning of Entities and Events in Procedural Texts",
  author = "Zhang, Li  and
    Xu, Hainiu  and
    Yang, Yue  and
    Zhou, Shuyan  and
    You, Weiqiu  and
    Arora, Manni  and
    Callison-burch, Chris",
  booktitle = "Findings of the Association for Computational Linguistics: EACL 2023",
  month = may,
  year = "2023",
  address = "Dubrovnik, Croatia",
  publisher = "Association for Computational Linguistics",
  url = "https://aclanthology.org/2023.findings-eacl.31",
  pages = "415--431",
  abstract = "Entities and events are crucial to natural language reasoning and common in procedural texts. Existing work has focused either exclusively on entity state tracking (e.g., whether a pan is hot) or on event reasoning (e.g., whether one would burn themselves by touching the pan), while these two tasks are often causally related. We propose CREPE, the first benchmark on causal reasoning of event plausibility and entity states. We show that most language models, including GPT-3, perform close to chance at .35 F1, lagging far behind human at .87 F1. We boost model performance to .59 F1 by creatively representing events as programming languages while prompting language models pretrained on code. By injecting the causal relations between entities and events as intermediate reasoning steps in our representation, we further boost the performance to .67 F1. Our findings indicate not only the challenge that CREPE brings for language models, but also the efficacy of code-like prompting combined with chain-of-thought prompting for multihop event reasoning.",
}
          </pre> </div>
          <p class="w3-text-gray">[17] <i>Unsupervised Entity Linking with Guided Summarization and Multiple Choice Selection</i>; Young Min Cho<span class="tooltip">^<span class="tooltiptext">Mentored student</span></span>, <b>Li Zhang</b> and Chris Callison-Burch; in <a href="https://2022.emnlp.org/" target="_blank">EMNLP 2022</a>.<a href="https://aclanthology.org/2022.emnlp-main.638/" target="_blank"><span class="w3-tag w3-white w3-border w3-border-black w3-text-black w3-round w3-hover-opacity">Paper</span></a> <span class="help w3-tag w3-white w3-border w3-border-black w3-text-black w3-round w3-hover-opacity" onclick="toggle('emnlp2022bib')">BibTeX</span> <a href="https://github.com/JeffreyCh0/SumMC" target="_blank"><span class="w3-tag w3-white w3-border w3-border-black w3-text-black w3-round w3-hover-opacity">Repo</span></a></p>
                   <div id="emnlp2022bib" class='hidden'> <pre style="overflow:auto">
@inproceedings{cho-etal-2022-unsupervised,
title = "Unsupervised Entity Linking with Guided Summarization and Multiple-Choice Selection",
author = "Cho, Young Min  and
Zhang, Li  and
Callison-Burch, Chris",
booktitle = "Proceedings of the 2022 Conference on Empirical Methods in Natural Language Processing",
month = dec,
year = "2022",
address = "Abu Dhabi, United Arab Emirates",
publisher = "Association for Computational Linguistics",
url = "https://aclanthology.org/2022.emnlp-main.638",
pages = "9394--9401",
abstract = "Entity linking, the task of linking potentially ambiguous mentions in texts to corresponding knowledge-base entities, is an important component for language understanding. We address two challenge in entity linking: how to leverage wider contexts surrounding a mention, and how to deal with limited training data. We propose a fully unsupervised model called SumMC that first generates a guided summary of the contexts conditioning on the mention, and then casts the task to a multiple-choice problem where the model chooses an entity from a list of candidates. In addition to evaluating our model on existing datasets that focus on named entities, we create a new dataset that links noun phrases from WikiHow to Wikidata. We show that our SumMC model achieves state-of-the-art unsupervised performance on our new dataset and on exiting datasets.",
}
    </pre> </div>
    <p class="w3-text-gray">[16] <i>GEMv2: Multilingual NLG Benchmarking in a Single Line of Code</i>; ... <b>Li Zhang</b>, Huaiyu Zhu, Siddhartha Brahma, Yunyao Li, ...; in <a href="https://2020.emnlp.org/" target="_blank">EMNLP 2022</a>.<a href="https://arxiv.org/abs/2206.11249" target="_blank"><span class="w3-tag w3-white w3-border w3-border-black w3-text-black w3-round w3-hover-opacity">Paper</span></a></p>
        <p class="w3-text-gray">[15] <i>Beyond the Imitation Game: Quantifying and extrapolating the capabilities of language models</i>; ... <b>Li Zhang</b><span class="tooltip">*<span class="tooltiptext">Equal contribution</span></span>, Qing Lyu<span class="tooltip">*<span class="tooltiptext">Equal contribution</span></span> and Chris Callison-Burch; in <a href="https://www.jmlr.org/tmlr/" target="_blank">TMLR</a>.<a href="https://arxiv.org/abs/2206.04615" target="_blank"><span class="w3-tag w3-white w3-border w3-border-black w3-text-black w3-round w3-hover-opacity">Paper</span></a></p>
        
    <p class="w3-text-gray">[13] <i>QuakerBot: A Household Dialog System Powered by Large Language Models</i>; Artemis Panagopoulou, Manni Arora<span class="tooltip">^<span class="tooltiptext">Mentored student</span></span>, <b>Li Zhang</b>, Dimitri Cugini, Weiqiu You, Yue Yang, Liyang Zhou<span class="tooltip">^<span class="tooltiptext">Mentored student</span></span>, Yuxuan Wang, Zhaoyi Hou<span class="tooltip">^<span class="tooltiptext">Mentored student</span></span>, Alyssa Hwang, Lara Martin, Sherry Shi, Chris Callison-Burch and Mark Yatskar; in <a href="https://www.amazon.science/alexa-prize/proceedings" target="_blank">Alexa Prize Proceedings 2022</a>.<a href="https://www.amazon.science/alexa-prize/proceedings/quakerbot-a-household-dialog-system-powered-by-large-language-models" target="_blank"><span class="w3-tag w3-white w3-border w3-border-black w3-text-black w3-round w3-hover-opacity">Paper</span></a> <span class="help w3-tag w3-white w3-border w3-border-black w3-text-black w3-round w3-hover-opacity" onclick="toggle('quakerbotbib')">BibTeX</span></p>
  <div id="quakerbotbib" class='hidden'> 
    <pre style="overflow:auto">
@Inproceedings{Pennsylvania2022,
author = {Panagopoulou, Artemis and Arora, Manni and Zhang, Li and Cugini, Dimitri and You, Weiqiu and Yang, Yue and Zhou, Liyang and Wang, Yuxuan and Hou, Zhaoyi and Hwang, Alyssa and Martin, Lara and Shi, Sherry and Callison-Burch, Chris and Yatskar, Mark},
title = {QuakerBot: A household dialog system powered by large language models},
year = {2022},
url = {https://www.amazon.science/alexa-prize/proceedings/quakerbot-a-household-dialog-system-powered-by-large-language-models},
booktitle = {Alexa Prize TaskBot Challenge Proceedings},
}
</pre> 
</div>  
<p class="w3-text-gray">[12] <i>Is "my favorite new movie" my favorite movie? Probing the Understanding of Recursive Noun Phrases</i>; Qing Lyu, Hua Zheng, Daoxin Li, <b>Li Zhang</b>, Marianna Apidianaki and Chris Callison-Burch; in <a href="https://2022.naacl.org/" target="_blank">NAACL 2022</a>.<a href="https://aclanthology.org/2022.naacl-main.388/" target="_blank"><span class="w3-tag w3-white w3-border w3-border-black w3-text-black w3-round w3-hover-opacity">Paper</span></a> <span class="help w3-tag w3-white w3-border w3-border-black w3-text-black w3-round w3-hover-opacity" onclick="toggle('npbib')">BibTeX</span> <a href="https://github.com/veronica320/Recursive-NPs" target="_blank"><span class="w3-tag w3-white w3-border w3-border-black w3-text-black w3-round w3-hover-opacity">Repo</span></a></p>
          <div id="npbib" class='hidden'> <pre style="overflow:auto">
@inproceedings{lyu-etal-2022-favorite,
    title = "Is {``}My Favorite New Movie{''} My Favorite Movie? Probing the Understanding of Recursive Noun Phrases",
    author = "Lyu, Qing  and
      Hua, Zheng  and
      Li, Daoxin  and
      Zhang, Li  and
      Apidianaki, Marianna  and
      Callison-Burch, Chris",
    booktitle = "Proceedings of the 2022 Conference of the North American Chapter of the Association for Computational Linguistics: Human Language Technologies",
    month = jul,
    year = "2022",
    address = "Seattle, United States",
    publisher = "Association for Computational Linguistics",
    url = "https://aclanthology.org/2022.naacl-main.388",
    pages = "5286--5302",
    abstract = "Recursive noun phrases (NPs) have interesting semantic properties. For example, {``}my favorite new movie{''} is not necessarily my favorite movie, whereas {``}my new favorite movie{''} is. This is common sense to humans, yet it is unknown whether language models have such knowledge. We introduce the Recursive Noun Phrase Challenge (RNPC), a dataset of three textual inference tasks involving textual entailment and event plausibility comparison, precisely targeting the understanding of recursive NPs. When evaluated on RNPC, state-of-the-art Transformer models only perform around chance. Still, we show that such knowledge is learnable with appropriate data. We further probe the models for relevant linguistic features that can be learned from our tasks, including modifier semantic category and modifier scope. Finally, models trained on RNPC achieve strong zero-shot performance on an extrinsic Harm Detection evaluation task, showing the usefulness of the understanding of recursive NPs in downstream applications.",
}
        </pre> </div> 
        <p class="w3-text-gray">[11] <i>Label Definitions Improve Semantic Role Labeling</i>; <b>Li Zhang</b>, Ishan Jindal, Yunyao Li; in <a href="https://2022.naacl.org/" target="_blank">NAACL 2022</a>.<a href="https://aclanthology.org/2022.naacl-main.411/" target="_blank"><span class="w3-tag w3-white w3-border w3-border-black w3-text-black w3-round w3-hover-opacity">Paper</span></a> <span class="help w3-tag w3-white w3-border w3-border-black w3-text-black w3-round w3-hover-opacity" onclick="toggle('srlbib')">BibTeX</span> <a href="https://github.com/System-T/LabelAwareSRL" target="_blank"><span class="w3-tag w3-white w3-border w3-border-black w3-text-black w3-round w3-hover-opacity">Repo</span></a></p>
            <div id="srlbib" class='hidden'> <pre style="overflow:auto">
@inproceedings{zhang-etal-2022-label-definitions,
    title = "Label Definitions Improve Semantic Role Labeling",
    author = "Zhang, Li  and
      Jindal, Ishan  and
      Li, Yunyao",
    booktitle = "Proceedings of the 2022 Conference of the North American Chapter of the Association for Computational Linguistics: Human Language Technologies",
    month = jul,
    year = "2022",
    address = "Seattle, United States",
    publisher = "Association for Computational Linguistics",
    url = "https://aclanthology.org/2022.naacl-main.411",
    pages = "5613--5620",
    abstract = "Argument classification is at the core of Semantic Role Labeling. Given a sentence and the predicate, a semantic role label is assigned to each argument of the predicate. While semantic roles come with meaningful definitions, existing work has treated them as symbolic. Learning symbolic labels usually requires ample training data, which is frequently unavailable due to the cost of annotation. We instead propose to retrieve and leverage the definitions of these labels from the annotation guidelines. For example, the verb predicate {``}work{''} has arguments defined as {``}worker{''}, {``}job{''}, {``}employer{''}, etc. Our model achieves state-of-the-art performance on the CoNLL09 dataset injected with label definitions given the predicate senses. The performance improvement is even more pronounced in low-resource settings when training data is scarce.",
}
          </pre> </div> 
          <p class="w3-text-gray">[10] <i>Show Me More Details: Discovering Hierarchies of Procedures from Semi-structured Web Data</i>; Shuyan Zhou<span class="tooltip">*<span class="tooltiptext">Equal contribution</span></span>, <b>Li Zhang</b><span class="tooltip">*<span class="tooltiptext">Equal contribution</span></span>, Yue Yang, Qing Lyu, Pengcheng Yin, Chris Callison-Burch and Graham Neubig; in <a href="https://www.2022.aclweb.org/" target="_blank">ACL 2022.</a><a href="https://aclanthology.org/2022.acl-long.214/" target="_blank"><span class="w3-tag w3-white w3-border w3-border-black w3-text-black w3-round w3-hover-opacity">Paper</span></a> <span class="help w3-tag w3-white w3-border w3-border-black w3-text-black w3-round w3-hover-opacity" onclick="toggle('hierarchybib')">BibTeX</span> <a href="https://wikihow-hierarchy.github.io/" target="_blank"><span class="w3-tag w3-white w3-border w3-border-black w3-text-black w3-round w3-hover-opacity">Demo</span></a> <a href="https://github.com/shuyanzhou/wikihow_hierarchy" target="_blank"><span class="w3-tag w3-white w3-border w3-border-black w3-text-black w3-round w3-hover-opacity">Repo</span></a></p>
        <div id="hierarchybib" class='hidden'> 
          <pre style="overflow:auto">
@inproceedings{zhou-etal-2022-show,
title = "Show Me More Details: Discovering Hierarchies of Procedures from Semi-structured Web Data",
author = "Zhou, Shuyan  and
  Zhang, Li  and
  Yang, Yue  and
  Lyu, Qing  and
  Yin, Pengcheng  and
  Callison-Burch, Chris  and
  Neubig, Graham",
booktitle = "Proceedings of the 60th Annual Meeting of the Association for Computational Linguistics (Volume 1: Long Papers)",
month = may,
year = "2022",
address = "Dublin, Ireland",
publisher = "Association for Computational Linguistics",
url = "https://aclanthology.org/2022.acl-long.214",
pages = "2998--3012",
abstract = "Procedures are inherently hierarchical. To {``}make videos{''}, one may need to {``}purchase a camera{''}, which in turn may require one to {``}set a budget{''}. While such hierarchical knowledge is critical for reasoning about complex procedures, most existing work has treated procedures as shallow structures without modeling the parent-child relation. In this work, we attempt to construct an open-domain hierarchical knowledge-base (KB) of procedures based on wikiHow, a website containing more than 110k instructional articles, each documenting the steps to carry out a complex procedure. To this end, we develop a simple and efficient method that links steps (e.g., {``}purchase a camera{''}) in an article to other articles with similar goals (e.g., {``}how to choose a camera{''}), recursively constructing the KB. Our method significantly outperforms several strong baselines according to automatic evaluation, human judgment, and application to downstream tasks such as instructional video retrieval.",
}
      </pre> 
    </div>  
    <p class="w3-text-gray">[9] <i>Visual Goal-Step Inference using wikiHow</i>; Yue Yang, Artemis Panagopoulou, Qing Lyu, <b>Li Zhang</b>, Mark Yatskar and Chris Callison-Burch;  In <a href="https://2021.emnlp.org/" target="_blank">EMNLP 2021</a>.<a href="https://aclanthology.org/2021.emnlp-main.165/" target="_blank"><span class="w3-tag w3-white w3-border w3-border-black w3-text-black w3-round w3-hover-opacity">Paper</span></a> <span class="help w3-tag w3-white w3-border w3-border-black w3-text-black w3-round w3-hover-opacity" onclick="toggle('emnlp2021bib')">BibTeX</span></p>
          <div id="emnlp2021bib" class='hidden'> <pre style="overflow:auto">
@inproceedings{yang-etal-2021-visual,
title = "Visual Goal-Step Inference using wiki{H}ow",
author = "Yang, Yue  and
  Panagopoulou, Artemis  and
  Lyu, Qing  and
  Zhang, Li  and
  Yatskar, Mark  and
  Callison-Burch, Chris",
booktitle = "Proceedings of the 2021 Conference on Empirical Methods in Natural Language Processing",
month = nov,
year = "2021",
address = "Online and Punta Cana, Dominican Republic",
publisher = "Association for Computational Linguistics",
url = "https://aclanthology.org/2021.emnlp-main.165",
pages = "2167--2179",
abstract = "Understanding what sequence of steps are needed to complete a goal can help artificial intelligence systems reason about human activities. Past work in NLP has examined the task of goal-step inference for text. We introduce the visual analogue. We propose the Visual Goal-Step Inference (VGSI) task, where a model is given a textual goal and must choose which of four images represents a plausible step towards that goal. With a new dataset harvested from wikiHow consisting of 772,277 images representing human actions, we show that our task is challenging for state-of-the-art multimodal models. Moreover, the multimodal representation learned from our data can be effectively transferred to other datasets like HowTo100m, increasing the VGSI accuracy by 15 - 20{\%}. Our task will facilitate multimodal reasoning about procedural events.",
}
        </pre> </div>
        <p class="w3-text-gray">[8] <i>Goal-Oriented Script Construction</i>; Qing Lyu<span class="tooltip">*<span class="tooltiptext">Equal contribution</span></span>, <b>Li Zhang</b><span class="tooltip">*<span class="tooltiptext">Equal contribution</span></span> and Chris Callison-Burch; in <a href="https://inlg2021.github.io/index.html" target="_blank">INLG 2021</a>.<a href="https://aclanthology.org/2021.inlg-1.19/" target="_blank"><span class="w3-tag w3-white w3-border w3-border-black w3-text-black w3-round w3-hover-opacity">Paper</span></a> <span class="help w3-tag w3-white w3-border w3-border-black w3-text-black w3-round w3-hover-opacity" onclick="toggle('inlg2021bib')">BibTeX</span> <a href="https://github.com/veronica320/wikihow-GOSC" target="_blank"><span class="w3-tag w3-white w3-border w3-border-black w3-text-black w3-round w3-hover-opacity">Repo</span></a></p>
                     <div id="inlg2021bib" class='hidden'> <pre style="overflow:auto">
@inproceedings{lyu-etal-2021-goal,
title = "Goal-Oriented Script Construction",
author = "Lyu, Qing  and
Zhang, Li  and
Callison-Burch, Chris",
booktitle = "Proceedings of the 14th International Conference on Natural Language Generation",
month = aug,
year = "2021",
address = "Aberdeen, Scotland, UK",
publisher = "Association for Computational Linguistics",
url = "https://aclanthology.org/2021.inlg-1.19",
pages = "184--200",
abstract = "The knowledge of scripts, common chains of events in stereotypical scenarios, is a valuable asset for task-oriented natural language understanding systems. We propose the Goal-Oriented Script Construction task, where a model produces a sequence of steps to accomplish a given goal. We pilot our task on the first multilingual script learning dataset supporting 18 languages collected from wikiHow, a website containing half a million how-to articles. For baselines, we consider both a generation-based approach using a language model and a retrieval-based approach by first retrieving the relevant steps from a large candidate pool and then ordering them. We show that our task is practical, feasible but challenging for state-of-the-art Transformer models, and that our methods can be readily deployed for various other datasets and domains with decent zero-shot performance.",
}
      </pre> </div>
            <p class="w3-text-gray">[7] <i>Intent Detection with WikiHow</i>; <b>Li Zhang</b>, Qing Lyu, Chris Callison-Burch; in <a href="http://www.aacl2020.org/" target="_blank">AACL-IJCNLP 2020</a>.<a href="https://www.aclweb.org/anthology/2020.aacl-main.35/" target="_blank"><span class="w3-tag w3-white w3-border w3-border-black w3-text-black w3-round w3-hover-opacity">Paper</span></a> <span class="help w3-tag w3-white w3-border w3-border-black w3-text-black w3-round w3-hover-opacity" onclick="toggle('aacl2020bib')">BibTeX</span> <a href="https://github.com/zharry29/wikihow-intent" target="_blank"><span class="w3-tag w3-white w3-border w3-border-black w3-text-black w3-round w3-hover-opacity">Repo</span></a></p>
            <div id="aacl2020bib" class='hidden'> <pre style="overflow:auto">
@inproceedings{zhang-etal-2020-intent,
  title = "Intent Detection with {W}iki{H}ow",
  author = "Zhang, Li  and
    Lyu, Qing  and
    Callison-Burch, Chris",
  booktitle = "Proceedings of the 1st Conference of the Asia-Pacific Chapter of the Association for Computational Linguistics and the 10th International Joint Conference on Natural Language Processing",
  month = dec,
  year = "2020",
  address = "Suzhou, China",
  publisher = "Association for Computational Linguistics",
  url = "https://www.aclweb.org/anthology/2020.aacl-main.35",
  pages = "328--333",
  abstract = "Modern task-oriented dialog systems need to reliably understand users{'} intents. Intent detection is even more challenging when moving to new domains or new languages, since there is little annotated data. To address this challenge, we present a suite of pretrained intent detection models which can predict a broad range of intended goals from many actions because they are trained on wikiHow, a comprehensive instructional website. Our models achieve state-of-the-art results on the Snips dataset, the Schema-Guided Dialogue dataset, and all 3 languages of the Facebook multilingual dialog datasets. Our models also demonstrate strong zero- and few-shot performance, reaching over 75{\%} accuracy using only 100 training examples in all datasets.",
}
          </pre> </div> 

          <p class="w3-text-gray">[6] <i>Reasoning about Goals, Steps, and Temporal Ordering with WikiHow</i>; <b>Li Zhang</b><span class="tooltip">*<span class="tooltiptext">Equal contribution</span></span>, Qing Lyu<span class="tooltip">*<span class="tooltiptext">Equal contribution</span></span> and Chris Callison-Burch; in <a href="https://2020.emnlp.org/" target="_blank">EMNLP 2020</a>.<a href="https://www.aclweb.org/anthology/2020.emnlp-main.374/" target="_blank"><span class="w3-tag w3-white w3-border w3-border-black w3-text-black w3-round w3-hover-opacity">Paper</span></a> <span class="help w3-tag w3-white w3-border w3-border-black w3-text-black w3-round w3-hover-opacity" onclick="toggle('emnlp2020-1bib')">BibTeX</span> <a href="https://github.com/zharry29/wikihow-goal-step" target="_blank"><span class="w3-tag w3-white w3-border w3-border-black w3-text-black w3-round w3-hover-opacity">Repo</span></a></p>
            <div id="emnlp2020-1bib" class='hidden'> <pre style="overflow:auto">
@inproceedings{zhang-etal-2020-reasoning,
  title = "Reasoning about Goals, Steps, and Temporal Ordering with {W}iki{H}ow",
  author = "Zhang, Li  and
    Lyu, Qing  and
    Callison-Burch, Chris",
  booktitle = "Proceedings of the 2020 Conference on Empirical Methods in Natural Language Processing (EMNLP)",
  month = nov,
  year = "2020",
  address = "Online",
  publisher = "Association for Computational Linguistics",
  url = "https://www.aclweb.org/anthology/2020.emnlp-main.374",
  pages = "4630--4639",
}
          </pre> </div> 
          <p class="w3-text-gray">[5] <i>Small but Mighty: New Benchmarks for Split and Rephrase</i>; <b>Li Zhang</b>, Huaiyu Zhu, Siddhartha Brahma and Yunyao Li; in <a href="https://2020.emnlp.org/" target="_blank">EMNLP 2020</a>; a part of the <a href="https://gem-benchmark.com" target="_blank">GEM Benchmark</a>.<a href="https://www.aclweb.org/anthology/2020.emnlp-main.91/" target="_blank"><span class="w3-tag w3-white w3-border w3-border-black w3-text-black w3-round w3-hover-opacity">Paper</span></a> <span class="help w3-tag w3-white w3-border w3-border-black w3-text-black w3-round w3-hover-opacity" onclick="toggle('emnlp2020-2bib')">BibTeX</span> <a href="https://github.com/System-T/TextSimplification" target="_blank"><span class="w3-tag w3-white w3-border w3-border-black w3-text-black w3-round w3-hover-opacity">Repo</span></a></p>
          <div id="emnlp2020-2bib" class='hidden'> <pre style="overflow:auto">
@inproceedings{zhang-etal-2020-small,
title = "Small but Mighty: New Benchmarks for Split and Rephrase",
author = "Zhang, Li  and
  Zhu, Huaiyu  and
  Brahma, Siddhartha  and
  Li, Yunyao",
booktitle = "Proceedings of the 2020 Conference on Empirical Methods in Natural Language Processing (EMNLP)",
month = nov,
year = "2020",
address = "Online",
publisher = "Association for Computational Linguistics",
url = "https://www.aclweb.org/anthology/2020.emnlp-main.91",
pages = "1198--1205",
}      
        </pre> </div> 
        
          <p class="w3-text-gray">[4] <i>Multi-Label Transfer Learning for Multi-Relational Semantic Similarity</i>; <b>Li Zhang</b>, Steven R. Wilson and Rada Mihalcea; In <a href="https://starsem.org/2019/" target="_blank">*SEM 2019</a>. <a href="https://www.aclweb.org/anthology/S19-1005" target="_blank"><span class="w3-tag w3-white w3-border w3-border-black w3-text-black w3-round w3-hover-opacity">Paper</span></a> <span class="help w3-tag w3-white w3-border w3-border-black w3-text-black w3-round w3-hover-opacity" onclick="toggle('sem2019bib')">BibTeX</span> <a href="docs/starSEM2019-presentation.pdf" target="_blank"><span class="w3-tag w3-white w3-border w3-border-black w3-text-black w3-round w3-hover-opacity">Slides</span></a></p>
          <div id="sem2019bib" class='hidden'> <pre style="overflow:auto">
@inproceedings{zhang-etal-2019-multi,
title = "Multi-Label Transfer Learning for Multi-Relational Semantic Similarity",
author = "Zhang, Li  and
  Wilson, Steven  and
  Mihalcea, Rada",
booktitle = "Proceedings of the Eighth Joint Conference on Lexical and Computational Semantics (*{SEM} 2019)",
month = jun,
year = "2019",
address = "Minneapolis, Minnesota",
publisher = "Association for Computational Linguistics",
url = "https://www.aclweb.org/anthology/S19-1005",
pages = "44--50",
abstract = "Multi-relational semantic similarity datasets define the semantic relations between two short texts in multiple ways, e.g., similarity, relatedness, and so on. Yet, all the systems to date designed to capture such relations target one relation at a time. We propose a multi-label transfer learning approach based on LSTM to make predictions for several relations simultaneously and aggregate the losses to update the parameters. This multi-label regression approach jointly learns the information provided by the multiple relations, rather than treating them as separate tasks. Not only does this approach outperform the single-task approach and the traditional multi-task learning approach, but it also achieves state-of-the-art performance on all but one relation of the Human Activity Phrase dataset.",
}
        </pre> </div>
          <p class="w3-text-gray">[3] <i>Direct Network Transfer: Transfer Learning of Sentence Embeddings for Semantic Similarity</i>; <b>Li Zhang</b>, Steven R. Wilson and Rada Mihalcea; in arXiv pre-print; presented at <a href="https://www.kellogg.northwestern.edu/news-events/conference/ic2s2/2018.aspx" target='_blank'>IC2S2 2018</a>.<a href="https://arxiv.org/abs/1804.07835" target="_blank"><span class="w3-tag w3-white w3-border w3-border-black w3-text-black w3-round w3-hover-opacity">Paper</span></a> <span class="help w3-tag w3-white w3-border w3-border-black w3-text-black w3-round w3-hover-opacity" onclick="toggle('dntbib')">BibTeX</span> <a href="docs/ic2s2_18_activities.pptx.pdf"><span class="w3-tag w3-white w3-border w3-border-black w3-text-black w3-round w3-hover-opacity">Poster</span></a></p>
          <div id="dntbib" class='hidden'> <pre style="overflow:auto">
@misc{zhang2018direct,
title={Direct Network Transfer: Transfer Learning of Sentence Embeddings for Semantic Similarity},
author={Li Zhang and Steven R. Wilson and Rada Mihalcea},
year={2018},
eprint={1804.07835},
archivePrefix={arXiv},
primaryClass={cs.CL}
}
        </pre> </div>
        <p class="w3-text-gray">[2] <i>Entity and Event Extraction from Scratch Using Minimal Training Data</i>; Laura Burdick, Steven R. Wilson, Oana Ignat, Charles F. Welch, <b>Li Zhang</b>, Mingzhe Wang, Jia Deng and Rada Mihalcea; in <a href="http://www.tac.mta.ca/tac/" target="_blank">TAC 2018.<a href="https://tac.nist.gov/publications/2018/participant.papers/TAC2018.Michigan.proceedings.pdf" target="_blank"><span class="w3-tag w3-white w3-border w3-border-black w3-text-black w3-round w3-hover-opacity">Paper</span></a> <span class="help w3-tag w3-white w3-border w3-border-black w3-text-black w3-round w3-hover-opacity" onclick="toggle('tac2018bib')">BibTeX</span> <a href="docs/ic2s2_18_activities.pptx.pdf"><span class="w3-tag w3-white w3-border w3-border-black w3-text-black w3-round w3-hover-opacity">Poster</span></a></p>
            <div id="tac2018bib" class='hidden'> <pre style="overflow:auto">
@article{Burdick2018EntityAE,
title={Entity and Event Extraction from Scratch Using Minimal Training Data},
author={Laura Burdick and Steven R. Wilson and Oana Ignat and Charles F Welch and Li Zhang and Mingzhe Wang and Jia Deng and Rada Mihalcea},
journal={Theory and Applications of Categories},
year={2018}
}
          </pre> </div>
            <p class="w3-text-gray">[1] <i>Improving Text-to-SQL Evaluation Methodology</i>; Catherine Finegan-Dollak, Jonathan K. Kummerfeld,
         <b>Li Zhang</b>, Karthik Ramanathan Dhanalakshmi
         Ramanathan, Sesh Sadasivam, Rui Zhang and Dragomir
         Radev; in <a href='https://acl2018.org/' target="_blank">ACL 2018</a>.<a href="https://www.aclweb.org/anthology/P18-1033/" target="_blank"><span class="w3-tag w3-white w3-border w3-border-black w3-text-black w3-round w3-hover-opacity">Paper</span></a> <span class="help w3-tag w3-white w3-border w3-border-black w3-text-black w3-round w3-hover-opacity" onclick="toggle('acl2018bib')">BibTeX</span> <a href="https://github.com/jkkummerfeld/text2sql-data"><span class="w3-tag w3-white w3-border w3-border-black w3-text-black w3-round w3-hover-opacity">Repo</span></a> <a href="docs/acl2018_poster - final.pdf"><span class="w3-tag w3-white w3-border w3-border-black w3-text-black w3-round w3-hover-opacity">Poster</span></a></p>
            <div id="acl2018bib" class='hidden'> <pre style="overflow:auto">
@InProceedings{acl18sql,
  author    = {Catherine Finegan-Dollak\*  and  Jonathan K. Kummerfeld\*  and  Li Zhang  and  Karthik Ramanathan  and  Sesh Sadasivam  and  Rui Zhang  and  Dragomir Radev},
  title     = {Improving Text-to-SQL Evaluation Methodology},
  booktitle = {Proceedings of the 56th Annual Meeting of the Association for Computational Linguistics (Volume 1: Long Papers)},
  shortvenue = {ACL},
  month     = {July},
  year      = {2018},
  address   = {Melbourne, Victoria, Australia},
  pages     = {351--360},
  abstract  = {To be informative, an evaluation must measure how well systems generalize to realistic unseen data. We identify limitations of and propose improvements to current evaluations of text-to-SQL systems. First, we compare human-generated and automatically generated questions, characterizing properties of queries necessary for real-world applications. To facilitate evaluation on multiple datasets, we release standardized and improved versions of seven existing datasets and one new text-to-SQL dataset. Second, we show that the current division of data into training and test sets measures robustness to variations in the way questions are asked, but only partially tests how well systems generalize to new queries; therefore, we propose a complementary dataset split for evaluation of future work. Finally, we demonstrate how the common practice of anonymizing variables during evaluation removes an important challenge of the task. Our observations highlight key difficulties, and our methodology enables effective measurement of future development.},
  url       = {http://aclweb.org/anthology/P18-1033},
  software  = {https://github.com/jkkummerfeld/text2sql-data},
  data      = {https://github.com/jkkummerfeld/text2sql-data},
}
          </pre> </div>
          </div>
      </div>
      <div id='show_less_button' class='hidden'>
        <hr>
        <button onclick="toggleCollapseResearch()" class='button'><span class="w3-tag w3-purple w3-round w3-hover-opacity">Collapse</span></button>
      </div>
      <br> 
          </div>
        
          <div class="w3-container w3-card-2 w3-white w3-margin-bottom">
            <h2 class="w3-text-grey w3-padding-16"><i class="fa fa-globe fa-fw w3-margin-right w3-xxlarge w3-text-purple"></i>Activities</h2>
            <ul>
              <li>May 7, 2025: Talk at <a href="https://www.brown.edu/" target="_blank">Brown University</a></li> 
              <li>Apr 3, 2025: Talk at <a href="https://www.temple.edu/" target="_blank">Temple University</a></li> 
              <li>Mar 19, 2025: Talk at <a href="https://msu.edu/" target="_blank">Michigan State University</a></li>
              <li>Mar 18, 2025: Talk at the <a href="https://umich.edu/" target="_blank">University of Michigan</a></li>
              <li>Mar 4, 2025: Talk at <a href="https://knowledgeable-lm.github.io/" target="_blank">AAAI 2025 workshop: Towards Knowledgeable Foundation Models</a></li>
              <li>Feb 27, 2025: Attending <a href="https://2024.naacl.org/" target="_blank">AAAI 2025</a></li>
              <li>Feb 21, 2025: Talk at <a href="https://allenai.org/" target="_blank">Allen Institute for AI</a></li> 
              <li>Feb 17, 2025: Talk at the <a href="https://www.upenn.edu/" target="_blank">University of Pennsylvania</a></li> 
              <li>Dec 1, 2024: Join <a href="https://drexel.edu/" target="_blank">Drexel University</a> as an assistant professor</li> 
              <li>Oct 1, 2024: Talk at <a href="https://www.stonybrook.edu/" target="_blank">Adobe World Headquarters</a></li> 
              <li>Sept 24, 2024: Talk at <a href="https://www.stonybrook.edu/" target="_blank">Stony Brook University</a></li> 
              <li>Aug 22, 2024: Talk at <a href="https://www.stanford.edu/" target="_blank">Stanford University</a></li> 
              <li>June 21, 2024: Presenting at <a href="https://sites.google.com/view/starsem2024" target="_blank">*SEM 2024</a></li> 
              <li>June 18, 2024: Attending <a href="https://2024.naacl.org/" target="_blank">NAACL 2024</a></li>
              <li>Apr 18, 2024: Talk at the <a href="https://www.pitt.edu/" target="_blank">University of Pittsburgh</a></li> 
              <li>March 17, 2024: Presenting at <a href="https://2024.eacl.org/" target="_blank">EACL 2024</a></li>
              <li>March 13, 2024: Talk at <a href="https://drexel.edu/" target="_blank">Drexel University</a></li> 
              <li>March 8, 2024: Talk at <a href="https://www.brandeis.edu/" target="_blank">Brandeis University</a></li> 
              <li>March 5, 2024: Talk at <a href="https://umbc.edu/" target="_blank">University of Maryland, Baltimore County</a></li> 
              <li>Feb 1, 2024: Guest lecture at the <a href="https://umich.edu/" target="_blank">University of Michigan</a></li>
              <li>Jan 31, 2024: Talk at <a href="https://www.kcl.ac.uk/" target="_blank">King's College London</a></li>
              <li>Jan 30, 2024: Talk at the <a href="https://umich.edu/" target="_blank">University of Michigan</a></li>
            </ul>  
        <div id='show_more_button_act'>
          <button onclick="toggleMoreActivities()" class='button'><span class="w3-tag w3-purple w3-round w3-hover-opacity">See past activities</span></button>
        </div>
        <div id="more_activities" class='hidden'>
          <ul>
            <li>Dec 6, 2023: Attending <a href="https://2023.emnlp.org/" target="_blank">EMNLP 2023</a></li>
              <li>Nov 6, 2023: Attending <a href="https://devday.openai.com/" target="_blank">OpenAI DevDay</a></li>
              <li>Jul 12, 2023: Presenting at <a href="https://2023.aclweb.org/" target="_blank">ACL 2023</a></li>
              <li>May 2, 2023: Presenting at <a href="https://2023.eacl.org/" target="_blank">EACL 2023</a></li>
              <li>Feb 13, 2023: Presenting at <a href="https://aaai-23.aaai.org/" target="_blank">AAAI 2023</a></li>
              <li>Feb 8, 2023: Talk at <a href="https://www.cmu.edu/" target="_blank">Carnegie Mellon University</a></li>
              <li>Feb 7, 2023: Guest lecture at <a href="https://oakland.edu/index" target="_blank">Oakland University</a></li>
              <li>Feb 7, 2023: Talk at the <a href="https://umich.edu/" target="_blank">University of Michigan</a></li>
              <li>Dec 7, 2022: Presenting at <a href="https://2022.emnlp.org/" target="_blank">EMNLP 2022</a></li>
          </ul>
        </div>
        <div id='show_less_button' class='hidden'>
          <hr>
          <button onclick="toggleCollapseActivities()" class='button'><span class="w3-tag w3-purple w3-round w3-hover-opacity">Collapse</span></button>
        </div>
        <br> 
        </div>


          <div class="w3-container w3-card-2 w3-white w3-margin-bottom">
            <h2 class="w3-text-grey w3-padding-16"><i class="fa fa-headphones fa-fw w3-margin-right w3-xxlarge w3-text-purple"></i>Music</h2>
            <div class="w3-container">
              I am a drummer, producer and video content creator. I run a video channel with over 50,000 subscribers on <a href="https://space.bilibili.com/483770554" target="_blank">Bilibili</a> and <a href="https://www.youtube.com/c/HazStudio" target="_blank">YouTube</a>, primarily making cover songs from video game and anime soundtracks, in a variety of styles ranging from metal to jazz. I am proudly sponsored by Vater, Tama, Mackie, Alesis, NUX, Xvive and have collaborated with manufacturers of major video games such as Genshin Impact and Azur Lane. 
              <br>
              <a href="https://space.bilibili.com/483770554" target="_blank"><img src="image/1200px-Bilibili_Logo_Blue.svg.png" alt="bilibili" width="50"></a> &nbsp; <a href="https://www.youtube.com/c/HazStudio" target="_blank"><i class="fab fa-youtube w3-xxlarge w3-hover-opacity"></i></a> &nbsp; <a href="https://open.spotify.com/artist/6ntpoMsRdS6WWUiuGjvcsf" target="_blank"><i class="fab fa-spotify w3-xxlarge w3-hover-opacity"></i></a> &nbsp; <a href="https://music.apple.com/us/artist/haz-studio/1565038720" target="_blank"><i class="fab fa-apple w3-xxlarge w3-hover-opacity"></i></a> &nbsp; <a href="https://music.amazon.com/artists/B093NPZGC5/haz-studio" target="_blank"><i class="fab fa-amazon w3-xxlarge w3-hover-opacity"></i></a>
              <br><br>
              My videos are primarily either collaboration-work music videos or solo-work drum covers. 
              <br><br>
            <div>
              <iframe src="https://www.youtube.com/embed/videoseries?list=PLdoWMpbmbJL_qDspGQtEgAMj9iHam6qqh" title="YouTube video player" frameborder="0" allow="accelerometer; autoplay; clipboard-write; encrypted-media; gyroscope; picture-in-picture; web-share" allowfullscreen style="width:48%"></iframe> &nbsp;
              <iframe src="https://www.youtube.com/embed/videoseries?list=PLdoWMpbmbJL-6JgN_t7txxRDK__eXDG1-" title="YouTube video player" frameborder="0" allow="accelerometer; autoplay; clipboard-write; encrypted-media; gyroscope; picture-in-picture; web-share" allowfullscreen style="width:48%"></iframe>
            </div>
            
            <hr>
            My new album, <a href="https://youtu.be/ue8eqHTN-FM" target="_blank">Megidalon</a>, a collection of reimagined Persona soundracks, is available for streaming on all platforms! My two previous albums Dazzling Tales (reimagined Genshin Impact soundtracks) and A Doll's Lament (reimagined NieR soundtracks) are also available for listening on all major streaming platforms.
            <br>
            <br>
            <div><a href="https://www.youtube.com/playlist?list=OLAK5uy_kWf45u05pATzl9he1C1cOtbGhVO2c_4Y0" target="_blank"><img src="image/megidolaon.png" style="width:32%" alt="Megidalon"></a>&nbsp;&nbsp;&nbsp;<a href="https://youtube.com/playlist?list=OLAK5uy_nqihISozPD2kzVR62R-UtkyfTWSa0y0hk" target="_blank"><img src="image/DazzlingTales.png" style="width:32%" alt="Dazzling Tales"></a>&nbsp;&nbsp;&nbsp;<a href="https://youtube.com/playlist?list=OLAK5uy_l9vZRQ9R-copJuC2uskVyuc7-k2rRE5cQ" target="_blank"><img src="image/ADollsLament.png" style="width:32%" alt="A Doll's Lament"></a></div>
            <br><hr>
            I also engage in research of AI music generation, having published a paper on automatic drum composition in an AAAI 2023 workshop.

<p class="w3-text-gray">[18] <i>Language Models are Drummers: Drum Composition with Natural Language Pre-Training</i>; <b>Li Zhang</b> and Chris Callison-Burch; in <a href="https://creativeai-ws.github.io/" target="_blank">AAAI 2023 Workshop on Creative AI Across Modalities</a>.<a href="https://arxiv.org/abs/2301.01162" target="_blank"><span class="w3-tag w3-white w3-border w3-border-black w3-text-black w3-round w3-hover-opacity">Paper</span></a> <a href="https://github.com/zharry29/drums-with-llm" target="_blank"><span class="w3-tag w3-white w3-border w3-border-black w3-text-black w3-round w3-hover-opacity">Repo</span></a> <span class="help w3-tag w3-white w3-border w3-border-black w3-text-black w3-round w3-hover-opacity" onclick="toggle('lmdrumbib')">BibTeX</span></p>
            <div id="lmdrumbib" class='hidden'> <pre style="overflow:auto">
  @InProceedings{gpt3drum,
    author    = {Li Zhang  and  Chris Callison-Burch},
    title     = {Language Models are Drummers: Drum Composition with Natural Language Pre-Training},
    venue = {AAAI 2023 1st workshop on Creative AI across Modalities},
    month     = {February},
    year      = {2023},
    address   = {Washington, D.C., USA},
    abstract  = {Automatic music generation with artificial intelligence typically requires a large amount of data which is hard to obtain for many less common genres and musical instruments. To tackle this issue, we present ongoing work and preliminary findings on the possibility for deep models to transfer knowledge from language to music, by finetuning large language models pre-trained on a massive text corpus on only hundreds of MIDI files of drum performances. We show that by doing so, one of the largest, state-of-the-art models (GPT3) is capable of generating reasonable drum grooves, while models that are not pre-trained (Transformer) shows no such ability beyond naive repetition. Evaluating generated music is a challenging task, more so is evaluating drum grooves with little precedence in literature. Hence, we propose a tailored structural evaluation method and analyze drum grooves produced by GPT3 compared to those played by human professionals, exposing the strengths and weaknesses of such generation by language-to-music transfer. Our findings suggest that language-to-music transfer learning with large language models is viable and promising.},
    url       = {https://arxiv.org/abs/2301.01162},
    software  = {https://github.com/zharry29/drums-with-llm},
  }
          </pre>
          </div>  
          <br>
          </div>
          <!-- End Right Column -->
      </div>
      <!-- End Grid -->
    </div>
    <!-- End Page Container -->
  </div>
  <footer class="w3-container w3-purple w3-center w3-margin-top">
    <p>&copy; Li Zhang &middot; All rights reserved &middot; Powered by w3.css</p>
  </footer>
</body>

</html>